{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SO4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVa6dNhBY1Fc",
        "outputId": "6a3ff1da-2201-4983-cedf-3490f9c6d73c"
      },
      "source": [
        "!pip install scikit-multilearn\n",
        "!pip install scikit-multilearn --upgrade\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime\n",
        "from skmultilearn.adapt import mlknn\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.svm\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "text_path='/content/drive/MyDrive/colab_SMAI/test1.txt'\n",
        "df=pd.read_csv(text_path, names=['Id', 'Title','Body','Tag'],skiprows=1,nrows=1200000)\n",
        "df['Tag'] = df['Tag'].fillna(\"\")\n",
        "df['Tag'] = df['Tag'].astype(str)\n",
        "\n",
        "df['cleaned_tags']=df['Tag'].str.lower().replace('\\n','')\n",
        "df['cleaned_tags']=df['cleaned_tags'].apply(lambda x: str(x).encode('ascii', 'ignore').decode('ascii'))\n",
        "df['cleaned_tags']=df['cleaned_tags'].str.replace('\\d+', '')\n",
        "df['cleaned_tags']=df['cleaned_tags'].str.replace(' +', ' ')\n",
        "\n",
        "junk_chars = \"[]{}.-\"\n",
        "for c in junk_chars:\n",
        "  df['cleaned_tags']=df['cleaned_tags'].str.replace(c, '')\n",
        "\n",
        "qus_list=[]\n",
        "qus_with_code = 0\n",
        "len_before_preprocessing = 0 \n",
        "len_after_preprocessing = 0 \n",
        "cnt = 0\n",
        "for index,row in df.iterrows():\n",
        "    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"cleaned_tags\"]\n",
        "    if '<code>' in body:\n",
        "        qus_with_code+=1\n",
        "    len_before_preprocessing+=len(title) + len(body)\n",
        "    body=re.sub('<code>(.*?)</code>', '', body, flags=re.MULTILINE|re.DOTALL)\n",
        "    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n",
        "    title=title.encode('utf-8')\n",
        "    question=str(title)+\" \"+str(body)\n",
        "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    qus_list.append(question)\n",
        "    len_after_preprocessing += len(question)\n",
        "    cnt = cnt + 1\n",
        "    if (cnt%25000 == 0):\n",
        "      print(\"Qstn body processed : \", cnt)\n",
        "\n",
        "df[\"question\"] = qus_list\n",
        "avg_len_before_preprocessing=(len_before_preprocessing*1.0)/df.shape[0]\n",
        "avg_len_after_preprocessing=(len_after_preprocessing*1.0)/df.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-multilearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/1f/e6ff649c72a1cdf2c7a1d31eb21705110ce1c5d3e7e26b2cc300e1637272/scikit_multilearn-0.2.0-py3-none-any.whl (89kB)\n",
            "\r\u001b[K     |███▊                            | 10kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20kB 14.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40kB 20.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61kB 22.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: scikit-multilearn\n",
            "Successfully installed scikit-multilearn-0.2.0\n",
            "Requirement already up-to-date: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Qstn body processed :  25000\n",
            "Qstn body processed :  50000\n",
            "Qstn body processed :  75000\n",
            "Qstn body processed :  100000\n",
            "Qstn body processed :  125000\n",
            "Qstn body processed :  150000\n",
            "Qstn body processed :  175000\n",
            "Qstn body processed :  200000\n",
            "Qstn body processed :  225000\n",
            "Qstn body processed :  250000\n",
            "Qstn body processed :  275000\n",
            "Qstn body processed :  300000\n",
            "Qstn body processed :  325000\n",
            "Qstn body processed :  350000\n",
            "Qstn body processed :  375000\n",
            "Qstn body processed :  400000\n",
            "Qstn body processed :  425000\n",
            "Qstn body processed :  450000\n",
            "Qstn body processed :  475000\n",
            "Qstn body processed :  500000\n",
            "Qstn body processed :  525000\n",
            "Qstn body processed :  550000\n",
            "Qstn body processed :  575000\n",
            "Qstn body processed :  600000\n",
            "Qstn body processed :  625000\n",
            "Qstn body processed :  650000\n",
            "Qstn body processed :  675000\n",
            "Qstn body processed :  700000\n",
            "Qstn body processed :  725000\n",
            "Qstn body processed :  750000\n",
            "Qstn body processed :  775000\n",
            "Qstn body processed :  800000\n",
            "Qstn body processed :  825000\n",
            "Qstn body processed :  850000\n",
            "Qstn body processed :  875000\n",
            "Qstn body processed :  900000\n",
            "Qstn body processed :  925000\n",
            "Qstn body processed :  950000\n",
            "Qstn body processed :  975000\n",
            "Qstn body processed :  1000000\n",
            "Qstn body processed :  1025000\n",
            "Qstn body processed :  1050000\n",
            "Qstn body processed :  1075000\n",
            "Qstn body processed :  1100000\n",
            "Qstn body processed :  1125000\n",
            "Qstn body processed :  1150000\n",
            "Qstn body processed :  1175000\n",
            "Qstn body processed :  1200000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7hkfav00Ad5",
        "outputId": "dcd687b9-acab-40d1-80ec-22c9d5f08f50"
      },
      "source": [
        "preprocessed_df = df[[\"question\",\"cleaned_tags\"]]\n",
        "preprocessed_df.dropna(\n",
        "    axis=0,\n",
        "    how='any',\n",
        "    thresh=None,\n",
        "    subset=None,\n",
        "    inplace=True\n",
        ")\n",
        "preprocessed_df = preprocessed_df.dropna(subset=['question','cleaned_tags'])\n",
        "print(\"Shape of preprocessed data :\", preprocessed_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of preprocessed data : (1200000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzJmdIn1AIkJ",
        "outputId": "977a1ed5-3b2e-44f4-ebc7-767c72333e0d"
      },
      "source": [
        "preprocessed_df = df[[\"question\",\"cleaned_tags\"]]\n",
        "preprocessed_df = preprocessed_df[:30000]\n",
        "print(\"Shape of preprocessed data :\", preprocessed_df.shape)\n",
        "preprocessed_df.drop_duplicates()\n",
        "print(\"Shape of preprocessed data :\", preprocessed_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of preprocessed data : (30000, 2)\n",
            "Shape of preprocessed data : (30000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uEOzO4rY_rX"
      },
      "source": [
        "def tags_to_consider(n):\n",
        "    tag_i_sum = y_multilabel.sum(axis=0).tolist()[0]\n",
        "    sorted_tags_i = sorted(range(len(tag_i_sum)), key=lambda i: tag_i_sum[i], reverse=True)\n",
        "    yn_multilabel=y_multilabel[:,sorted_tags_i[:n]]\n",
        "    return yn_multilabel\n",
        "\n",
        "def questions_covered_fn(numb):\n",
        "    yn_multilabel = tags_to_consider(numb)\n",
        "    x= yn_multilabel.sum(axis=1)\n",
        "    return (np.count_nonzero(x==0))\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "y_multilabel = vectorizer.fit_transform(preprocessed_df['cleaned_tags'])\n",
        "\n",
        "questions_covered = []\n",
        "total_tags=y_multilabel.shape[1]\n",
        "total_qus=preprocessed_df.shape[0]\n",
        "for i in range(100, total_tags, 100):\n",
        "    questions_covered.append(np.round(((total_qus-questions_covered_fn(i))/total_qus)*100,3))\n",
        "\n",
        "yx_multilabel = tags_to_consider(100)\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.1,random_state = 42)\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=20000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\n",
        "X_train_multilabel = vectorizer.fit_transform(X_train['question'])\n",
        "X_test_multilabel = vectorizer.transform(X_test['question'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HwbWEh7AboK",
        "outputId": "8f637d74-bb81-4bfb-bf4c-20e7077d98a8"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "mlpc = MLPClassifier()\n",
        "mlpc.fit(X_train_multilabel, y_train)\n",
        "y_pred = mlpc.predict(X_test_multilabel)\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
        "print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
        "print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.362\n",
            "Macro f1 score : 0.4503418316564851\n",
            "Micro f1 scoore : 0.5083237184770066\n",
            "Hamming loss : 0.009943333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbgLgzc3BI4w",
        "outputId": "2f5b98c9-85ca-42a3-f505-f9547be20cda"
      },
      "source": [
        "rfc = OneVsRestClassifier(RandomForestClassifier())\n",
        "rfc.fit(X_train_multilabel, y_train)\n",
        "y_pred = rfc.predict(X_test_multilabel)\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
        "print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
        "print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy : 0.345\n",
            "Macro f1 score : 0.14284270624545806\n",
            "Micro f1 scoore : 0.3578315834583244\n",
            "Hamming loss : 0.00999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwKyUGGTcMVp",
        "outputId": "15b9b170-1afb-4e27-bb37-f76a5e39e076"
      },
      "source": [
        "cparam = [.65, .7, 1]\n",
        "for cval in cparam:\n",
        "  print(\" ------------- for : \"+ str(cval)+ \"---------------\")\n",
        "  clf_lsvc = OneVsRestClassifier(LinearSVC(C=cval))\n",
        "  clf_lsvc.fit(X_train_multilabel, y_train)\n",
        "  y_pred = clf_lsvc.predict(X_test_multilabel)\n",
        "  print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
        "  print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
        "  print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
        "  print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ------------- for : 0.65---------------\n",
            "Accuracy : 0.393\n",
            "Macro f1 score : 0.4071852374590177\n",
            "Micro f1 scoore : 0.4921465968586387\n",
            "Hamming loss : 0.009053333333333333\n",
            " ------------- for : 0.7---------------\n",
            "Accuracy : 0.392\n",
            "Macro f1 score : 0.40916113215014344\n",
            "Micro f1 scoore : 0.49357422238778176\n",
            "Hamming loss : 0.009063333333333333\n",
            " ------------- for : 1---------------\n",
            "Accuracy : 0.39166666666666666\n",
            "Macro f1 score : 0.42203608028576667\n",
            "Micro f1 scoore : 0.5041924899744804\n",
            "Hamming loss : 0.009066666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "h9GfjJs3d63G",
        "outputId": "ed27b045-6f60-4bec-ac7d-e8b12acdb049"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaling = MinMaxScaler(feature_range=(-1,1)).fit(X_train_multilabel)\n",
        "X_train_multilabel = scaling.transform(X_train_multilabel)\n",
        "X_test_multilabel = scaling.transform(X_test_multilabel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-746c24e92762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscaling\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_multilabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_train_multilabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_multilabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_test_multilabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_multilabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             raise TypeError(\"MinMaxScaler does not support sparse input. \"\n\u001b[0m\u001b[1;32m    369\u001b[0m                             \"Consider using MaxAbsScaler instead.\")\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MinMaxScaler does not support sparse input. Consider using MaxAbsScaler instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp6DX5jxZDtQ"
      },
      "source": [
        "model = OneVsRestClassifier(svm.SVC(kernel='poly', degree=3))\n",
        "model.fit(X_train_multilabel, y_train)\n",
        "y_pred = model.predict(X_test_multilabel)\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
        "print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
        "print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}