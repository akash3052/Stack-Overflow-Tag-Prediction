{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SO3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBqR8JDZTapd"
      },
      "source": [
        "!pip install scikit-multilearn\n",
        "!pip install scikit-multilearn --upgrade\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "from sqlalchemy import create_engine # database connection\n",
        "import datetime as dt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from datetime import datetime\n",
        "from skmultilearn.adapt import mlknn\n",
        "from skmultilearn.problem_transform import ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tags_to_consider(n):\n",
        "    tag_i_sum = y_multilabel.sum(axis=0).tolist()[0]\n",
        "    sorted_tags_i = sorted(range(len(tag_i_sum)), key=lambda i: tag_i_sum[i], reverse=True)\n",
        "    yn_multilabel=y_multilabel[:,sorted_tags_i[:n]]\n",
        "    return yn_multilabel\n",
        "\n",
        "def questions_covered_fn(numb):\n",
        "    yn_multilabel = tags_to_consider(numb)\n",
        "    x= yn_multilabel.sum(axis=1)\n",
        "    return (np.count_nonzero(x==0))\n",
        "\n",
        "text_path='/content/drive/MyDrive/colab_SMAI/test1.txt'\n",
        "df=pd.read_csv(text_path, names=['Id', 'Title','Body','Tag'],skiprows=1,nrows=1200000)\n",
        "df['Tag'] = df['Tag'].fillna(\"\")\n",
        "df['Tag'] = df['Tag'].astype(str)\n",
        "\n",
        "df['cleaned_tags']=df['Tag'].str.lower().replace('\\n','')\n",
        "df['cleaned_tags']=df['cleaned_tags'].apply(lambda x: str(x).encode('ascii', 'ignore').decode('ascii'))\n",
        "df['cleaned_tags']=df['cleaned_tags'].str.replace('\\d+', '')\n",
        "df['cleaned_tags']=df['cleaned_tags'].str.replace(' +', ' ')\n",
        "\n",
        "junk_chars = \"[]{}.-\"\n",
        "for c in junk_chars:\n",
        "  df['cleaned_tags']=df['cleaned_tags'].str.replace(c, '')\n",
        "\n",
        "qus_list=[]\n",
        "qus_with_code = 0\n",
        "len_before_preprocessing = 0 \n",
        "len_after_preprocessing = 0 \n",
        "cnt = 0\n",
        "for index,row in df.iterrows():\n",
        "    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"cleaned_tags\"]\n",
        "    if '<code>' in body:\n",
        "        qus_with_code+=1\n",
        "    len_before_preprocessing+=len(title) + len(body)\n",
        "    body=re.sub('<code>(.*?)</code>', '', body, flags=re.MULTILINE|re.DOTALL)\n",
        "    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n",
        "    title=title.encode('utf-8')\n",
        "    question=str(title)+\" \"+str(body)\n",
        "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
        "    words=word_tokenize(str(question.lower()))\n",
        "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
        "    qus_list.append(question)\n",
        "    len_after_preprocessing += len(question)\n",
        "    cnt = cnt + 1\n",
        "    if (cnt%25000 == 0):\n",
        "      print(\"Qstn body processed : \", cnt)\n",
        "\n",
        "df[\"question\"] = qus_list\n",
        "avg_len_before_preprocessing=(len_before_preprocessing*1.0)/df.shape[0]\n",
        "avg_len_after_preprocessing=(len_after_preprocessing*1.0)/df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwE_za2w-YTP"
      },
      "source": [
        "preprocessed_df = df[[\"question\",\"cleaned_tags\"]]\n",
        "preprocessed_df = preprocessed_df[:30000]\n",
        "print(\"Shape of preprocessed data :\", preprocessed_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JSwBjCiT9V7"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
        "y_multilabel = vectorizer.fit_transform(preprocessed_df['cleaned_tags'])\n",
        "\n",
        "questions_covered = []\n",
        "total_tags=y_multilabel.shape[1]\n",
        "total_qus=preprocessed_df.shape[0]\n",
        "for i in range(100, total_tags, 100):\n",
        "    questions_covered.append(np.round(((total_qus-questions_covered_fn(i))/total_qus)*100,3))\n",
        "\n",
        "yx_multilabel = tags_to_consider(100)\n",
        "X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.1,random_state = 42)\n",
        "vectorizer = TfidfVectorizer(min_df=0.00009, max_features=20000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\n",
        "X_train_multilabel = vectorizer.fit_transform(X_train['question'])\n",
        "X_test_multilabel = vectorizer.transform(X_test['question'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-sGFv4bUcw0",
        "outputId": "4182f854-5b93-460d-a0c7-2c18423d4ecf"
      },
      "source": [
        "param_grid = {'estimator__C': [1, 100, 1000],\n",
        "              'estimator__kernel': ['poly'],\n",
        "              'estimator__degree':[1, 2, 3,4, 5]}\n",
        "svc = OneVsRestClassifier(SVC())\n",
        "rbf = model_selection.GridSearchCV(estimator=svc, param_grid=param_grid, verbose = 3)\n",
        "rbf.fit(X_train_multilabel, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "[CV] estimator__C=1, estimator__degree=1, estimator__gamma=scale, estimator__kernel=poly \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYhXEvUrU5-Q"
      },
      "source": [
        "print(rbf.best_params_)\n",
        "print(rbf.best_estimator_)\n",
        "y_pred = rbf.predict(X_test_multilabel)\n",
        "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\n",
        "print(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\n",
        "print(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\n",
        "print(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}